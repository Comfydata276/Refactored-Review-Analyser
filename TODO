- Add a time estimate for the analyse. How long per review, number of reviews * time 
- LLM Prompt engineering 
- output showing that batch processing is 100% working 
- Make application even more universal 
- handle output edge cases 
- Adaptability for different studys / columns 
- Modern UI 

TODO
x Fix llm processing for each vendor 
x Add a way to see and remove games in the game finder tab 
x Add a way to upload a new llm prompt file (need to make the llm prompt x file name a variable that can be changed when a new file is uploaded)
x Add all settings to the settings panel
x Remove the notification icon + user profile section in the top left
Update help page as needed
Wrap in electron and turn into a .exe file
Make application more universal and able to output and data type from the llm
x Remove the 'new' tag on the Results tab
Review python version of the app and add in any missing features
Add time estimate for analysis and scraping 
x Output to show batch processing is working
Add more robust llm status to the UI (what LLM is running and when)
Redo Session Stats


Make a matrix recommending models (compare different modedls/costs/token rate)
2x Reasoning Models
2x Non Reasoning Models 
2x Local Models 

Commands: 
cd Documents\GitHub\Refactored-Review-Analyser\frontend
npm run dev
cd Documents\GitHub\Refactored-Review-Analyser\backend
.\venv\Scripts\Activate
uvicorn app.main:app --reload --port 8000