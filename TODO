- Add a time estimate for the analyse. How long per review, number of reviews * time 
- LLM Prompt engineering 
- output showing that batch processing is 100% working 
- Make application even more universal 
- handle output edge cases 
- Adaptability for different studys / columns 
- Modern UI 

TODO
Fix llm processing for each vendor 
Add a way to see and remove games in the game finder tab 
Add a way to upload a new llm prompt file (need to make the llm prompt file name a variable that can be changed when a new file is uploaded)
Add all settings to the settings panel
Remove the notification icon + user profile section in the top left
Update help page as needed
Wrap in electron and turn into a .exe file
Make application more universal and able to output and data type from the llm
Remove the 'new' tag on the Results tab
Review python version of the app and add in any missing features
Add time estimate for analysis and scraping 
Output to sho batch processing is working


Make a matrix recommending models (compare different modedls/costs/token rate)
2x Reasoning Models
2x Non Reasoning Models 
2x Local Models 

Commands: 
cd Documents\GitHub\Refactored-Review-Analyser\frontend
npm run dev
cd Documents\GitHub\Refactored-Review-Analyser\backend
.\venv\Scripts\Activate
uvicorn app.main:app --reload --port 8000